{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 20000\n",
    "set_size = 60000\n",
    "batch_size = 2\n",
    "sample_dir = 'samples'\n",
    "save_dir = 'save'\n",
    "\n",
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Image processing\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "# Data loader\n",
    "# data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "#                                          batch_size=batch_size, \n",
    "#                                          shuffle=True)\n",
    "# Download MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transform, \n",
    "                               download=True)\n",
    "# Split original training set into 70% train and 30% validation\n",
    "train_size = int(0.7 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Select a random image from the new training set\n",
    "random_index = np.random.randint(len(train_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "mnist_images = train_dataloader.dataset.dataset.data\n",
    "mnist_labels = train_dataset.dataset.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 20000\n",
    "set_size = 60000\n",
    "batch_size = 2\n",
    "sample_dir = 'samples'\n",
    "save_dir = 'save'\n",
    "\n",
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Image processing\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "# Data loader\n",
    "# data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "#                                          batch_size=batch_size, \n",
    "#                                          shuffle=True)\n",
    "# Download MNIST dataset\n",
    "fashion_mnist_dataset = datasets.FashionMNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transform, \n",
    "                               download=True)\n",
    "# Split original training set into 70% train and 30% validation\n",
    "train_size = int(0.7 * len(fashion_mnist_dataset))\n",
    "val_size = len(fashion_mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(fashion_mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Select a random image from the new training set\n",
    "random_index = np.random.randint(len(train_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "fashion_mnist_images = train_dataloader.dataset.dataset.data\n",
    "fashion_mnist_labels = train_dataset.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                # Convert to tensor\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert RGB to Grayscale\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                               train=True, \n",
    "                               transform=transform, \n",
    "                               download=True)\n",
    "\n",
    "# # Split original training set into 70% train and 30% validation\n",
    "train_size = int(0.7 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# # Select a random image from the new training set\n",
    "# random_index = np.random.randint(len(train_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "cifar_images = torch.tensor(train_dataloader.dataset.dataset.data).permute(0, 3, 1, 2)  # Convert to Tensor & Normalize\n",
    "\n",
    "# cifar_images = train_dataloader.dataset.dataset.data\n",
    "cifar_labels = train_dataset.dataset.targets\n",
    "\n",
    "to_grayscale = transforms.Grayscale(num_output_channels=1)\n",
    "cifar_gray = torch.stack([to_grayscale(img) for img in cifar_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cdf_mapping(images):\n",
    "    \"\"\"\n",
    "    Compute the CDF mapping for each pixel across all images.\n",
    "    \"\"\"\n",
    "    \n",
    "    vector_size = images.shape[1]\n",
    "    images = images.view(-1, vector_size)  # Flatten images\n",
    "    sorted_pixels, _ = torch.sort(images, dim=0)\n",
    "    cdf = torch.linspace(0, 1, images.shape[0])\n",
    "    pixel_cdf_map = {}\n",
    "    \n",
    "    for i in range(vector_size):\n",
    "        pixel_cdf_map[i] = (sorted_pixels[:, i], cdf)\n",
    "    \n",
    "    return pixel_cdf_map\n",
    "\n",
    "def transform_original_to_uniform(image, pixel_cdf_map, reverse=False):\n",
    "    \"\"\"\n",
    "    Transform an image using the pixel CDF mapping.\n",
    "    \"\"\"\n",
    "    image = image.view(-1)  # Flatten image\n",
    "    transformed_image = torch.zeros_like(image, dtype=torch.float32)\n",
    "    \n",
    "    pixel_size = int(image.shape[0] ** 0.5)\n",
    "    for i in range(pixel_size * pixel_size):\n",
    "        pixel_values, cdf_values = pixel_cdf_map[i]\n",
    "        \n",
    "        if not reverse:\n",
    "            # Forward transformation (find CDF value for each pixel)\n",
    "            indices = torch.searchsorted(pixel_values, image[i])\n",
    "            transformed_image[i] = cdf_values[min(indices, len(cdf_values) - 1)]\n",
    "        else:\n",
    "            # Reverse transformation (find original pixel from CDF value)\n",
    "            indices = torch.searchsorted(cdf_values, image[i])\n",
    "            transformed_image[i] = pixel_values[min(indices, len(pixel_values) - 1)]\n",
    "    \n",
    "    transformed_image = transformed_image.view(pixel_size, pixel_size)  # Reshape back to image size\n",
    "    return transformed_image\n",
    "\n",
    "def compute_mean_and_covariance_in_data_space(images, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the covariance matrix of the dataset, ensuring it is positive definite.\n",
    "    \"\"\"\n",
    "    images = images.float()  # Ensure floating-point type\n",
    "    images = images.view(images.shape[0], -1)  # Flatten images\n",
    "    mean_vector = torch.mean(images, dim=0, keepdim=True)\n",
    "    centered_images = images - mean_vector\n",
    "    covariance_matrix = torch.matmul(centered_images.T, centered_images) / (images.shape[0] - 1)\n",
    "    \n",
    "    # Regularization: Add a small identity matrix to ensure positive definiteness\n",
    "    covariance_matrix += epsilon * torch.eye(covariance_matrix.shape[0])\n",
    "    \n",
    "    return covariance_matrix, mean_vector\n",
    "\n",
    "def compute_covariance_in_uniform_sapce(training_data, epsilon = 1e-5):\n",
    "    \"\"\"\n",
    "    Compute the covariance matrix Σ_u of uniforms obtained by applying marginal CDFs to a dataset of shape (n, d, d).\n",
    "    \n",
    "    Args:\n",
    "        training_data (torch.Tensor): Input tensor of shape (n, d, d), where:\n",
    "            - n: Number of samples\n",
    "            - d: Number of pixels per dimension\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Covariance matrix Σ_u of shape (d*d, d*d) for the uniforms.\n",
    "    \"\"\"\n",
    "    n, d, _ = training_data.shape\n",
    "    \n",
    "    # Flatten the images into vectors: shape (n, d*d)\n",
    "    flattened = training_data.view(n, -1)  # (n, d^2)\n",
    "    \n",
    "    # Compute ranks for each pixel across samples\n",
    "    ranks = torch.argsort(torch.argsort(flattened, dim=0), dim=0)  # (n, d^2)\n",
    "    \n",
    "    # Convert ranks to uniforms using (rank + 1)/(n + 1)\n",
    "    uniforms = (ranks.float() + 1.0) / (n + 1.0)  # (n, d^2)\n",
    "    \n",
    "    # Center the uniforms (mean of U(0,1) is 0.5)\n",
    "    centered = uniforms - 0.5  # (n, d^2)\n",
    "    \n",
    "    # Compute covariance matrix (unbiased estimator)\n",
    "    covariance_matrix_unifrom = torch.matmul(centered.T, centered) / (n - 1)  # (d^2, d^2)\n",
    "    \n",
    "    return covariance_matrix_unifrom + epsilon*torch.eye(covariance_matrix_unifrom.shape[0])\n",
    "\n",
    "def generate_random_uniform_images_from_gaussian(covariance_matrix_gaussian, num_samples = 1):\n",
    "    vector_size = covariance_matrix_gaussian.size(0)\n",
    "    num_pixels = int(vector_size**0.5)\n",
    "    mean = torch.zeros(vector_size, device=covariance_matrix_gaussian.device)\n",
    "    \n",
    "    # Create the multivariate normal distribution\n",
    "    mvn = torch.distributions.MultivariateNormal(\n",
    "        loc=mean,\n",
    "        covariance_matrix=covariance_matrix_gaussian\n",
    "    )\n",
    "    \n",
    "    # Generate samples\n",
    "    gaussian_samples = mvn.rsample((num_samples,))\n",
    "    uniform_samples = 0.5 * (1 + torch.erf(gaussian_samples / np.sqrt(2)))  # Convert Gaussian to Uniform [0,1]\n",
    "    return uniform_samples.reshape(num_samples, num_pixels, num_pixels)\n",
    "\n",
    "\n",
    "def generate_random_uniform_images(covariance_matrix, num_samples = 1):\n",
    "    \"\"\"\n",
    "    Generate samples from a multivariate normal distribution with mean 0 and given covariance matrix.\n",
    "    \n",
    "    Args:\n",
    "        num_samples (int): Number of samples to generate.\n",
    "        covariance_matrix (torch.Tensor): Covariance matrix of shape (num_pixels, num_pixels).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (num_samples, num_pixels) containing the samples.\n",
    "    \"\"\"\n",
    "    vector_size = covariance_matrix.size(0)\n",
    "    num_pixels = int(vector_size**0.5)\n",
    "    mean = torch.zeros(vector_size, device=covariance_matrix.device)\n",
    "    \n",
    "    # Create the multivariate normal distribution\n",
    "    mvn = torch.distributions.MultivariateNormal(\n",
    "        loc=mean,\n",
    "        covariance_matrix=covariance_matrix\n",
    "    )\n",
    "    \n",
    "    # Generate samples\n",
    "    gaussian_samples = mvn.rsample((num_samples,))\n",
    "    uniform_samples = 0.5 * (1 + torch.erf(gaussian_samples / np.sqrt(2)))  # Convert Gaussian to Uniform [0,1]\n",
    "    return uniform_samples.reshape(num_samples, num_pixels, num_pixels)\n",
    "\n",
    "def transform_dataset_to_uniform_and_gaussian_vectorized(dataset):\n",
    "    \"\"\"\n",
    "    Transform an entire dataset of images (tensor of shape (n, p, p)) so that for each pixel location,\n",
    "    the empirical distribution of pixel values becomes uniform in [0,1]. Then, using the relation\n",
    "    z = sqrt(2) * erfinv(2u - 1), convert the uniform dataset into a Gaussian distributed dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (torch.Tensor): Tensor of shape (n, p, p)\n",
    "        \n",
    "    Returns:\n",
    "        uniform_dataset (torch.Tensor): Transformed dataset with shape (n, p, p) with values in [0,1].\n",
    "        gaussian_dataset (torch.Tensor): Dataset transformed to be Gaussian distributed.\n",
    "    \"\"\"\n",
    "    n, p, _ = dataset.shape\n",
    "    # Flatten images: shape (n, p*p)\n",
    "    flat_data = dataset.view(n, -1)\n",
    "    \n",
    "    # Compute the rank for each pixel across the dataset via double argsort.\n",
    "    # First argsort sorts the values; the second argsort recovers the rank.\n",
    "    ranks = flat_data.argsort(dim=0).argsort(dim=0).float()\n",
    "    \n",
    "    # Normalize ranks to [0,1]\n",
    "    uniform_flat = ranks / (n - 1)\n",
    "    \n",
    "    # Reshape back to (n, p, p)\n",
    "    uniform_dataset = uniform_flat.view(n, p, p)\n",
    "    \n",
    "    # Convert the uniform dataset to Gaussian distributed data:\n",
    "    # For each pixel, apply: z = sqrt(2) * erfinv(2u - 1)\n",
    "    gaussian_dataset = torch.erfinv(2 * uniform_dataset - 1) * torch.sqrt(torch.tensor(2.0))\n",
    "    \n",
    "    return uniform_dataset, gaussian_dataset\n",
    "\n",
    "\n",
    "# def generate_uniform_iman_conover(covariance_matrix, n_samples=10000):\n",
    "#     \"\"\"\n",
    "#     Generate a sample from a d-dimensional distribution with Uniform(0,1) marginals \n",
    "#     whose covariance is approximately the given covariance_matrix.\n",
    "    \n",
    "#     The method uses the Iman–Conover procedure.\n",
    "    \n",
    "#     Args:\n",
    "#         covariance_matrix (np.ndarray): A d x d target covariance matrix.\n",
    "#             (For Uniform[0,1], the variance is 1/12, so the diagonal of covariance_matrix\n",
    "#             should be about 1/12.)\n",
    "#         n_samples (int): Number of samples to generate for the Iman–Conover adjustment.\n",
    "        \n",
    "#     Returns:\n",
    "#         sample (np.ndarray): A d x 1 column vector drawn from the adjusted Uniform(0,1) distribution.\n",
    "#     \"\"\"\n",
    "#     # Dimension (d) inferred from the covariance matrix\n",
    "#     d = covariance_matrix.shape[0]\n",
    "    \n",
    "#     # For Uniform[0,1], variance = 1/12. So the target correlation matrix is:\n",
    "#     R_target = covariance_matrix * 12.0\n",
    "    \n",
    "#     # Step 1: Generate independent Uniform(0,1) samples: shape (n_samples, d)\n",
    "#     U = np.random.uniform(0, 1, size=(n_samples, d))\n",
    "    \n",
    "#     # Step 2: Standardize each column of U\n",
    "#     U_std = (U - U.mean(axis=0)) / U.std(axis=0, ddof=1)\n",
    "    \n",
    "#     # Compute the empirical correlation matrix of U_std\n",
    "#     R_empirical = np.corrcoef(U_std, rowvar=False)\n",
    "    \n",
    "#     # Step 3: Compute Cholesky factors for the empirical and target correlation matrices\n",
    "#     L_empirical = np.linalg.cholesky(R_empirical)\n",
    "#     L_target = np.linalg.cholesky(R_target)\n",
    "    \n",
    "#     # Step 4: Compute the adjustment matrix\n",
    "#     A = L_target @ np.linalg.inv(L_empirical)\n",
    "    \n",
    "#     # Adjust the standardized samples\n",
    "#     Z = U_std @ A.T\n",
    "    \n",
    "#     # Step 5: For each variable (column), reassign values based on the ranks of Z,\n",
    "#     # so that the marginals remain Uniform(0,1) but the correlation structure is adjusted.\n",
    "#     U_adjusted = np.empty_like(U)\n",
    "#     for j in range(d):\n",
    "#         order = np.argsort(Z[:, j])\n",
    "#         sorted_vals = np.sort(U[:, j])\n",
    "#         U_adjusted[order, j] = sorted_vals\n",
    "    \n",
    "#     # Step 6: Choose one sample (e.g., the first row) and return it as a column vector.\n",
    "#     sample = U_adjusted[0, :].reshape(-1, 1)\n",
    "#     uniform_random_image = torch.tensor(sample).view(-1)\n",
    "#     return uniform_random_image\n",
    "\n",
    "def generate_uniform_from_gaussian(covariance_matrix_uniform, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Generates a column vector of uniform samples with the specified covariance matrix,\n",
    "    using a multivariate Gaussian transformation. Handles PyTorch tensors and allows\n",
    "    diagonal entries close to (but not exactly) 1/12.\n",
    "    \"\"\"\n",
    "    d = covariance_matrix_uniform.size(0)\n",
    "    \n",
    "    # Compute standard deviations for each uniform variable\n",
    "    sigma = torch.sqrt(torch.diag(covariance_matrix_uniform))  # Shape: (d,)\n",
    "    \n",
    "    # Compute correlation matrix for the uniforms\n",
    "    outer_sigma = torch.outer(sigma, sigma)\n",
    "    R_uniform = covariance_matrix_uniform / outer_sigma  # Shape: (d, d)\n",
    "    \n",
    "    # Compute Gaussian correlation matrix using adjusted formula\n",
    "    R_gaussian = 2 * torch.sin((math.pi / 6) * R_uniform)\n",
    "    R_gaussian.fill_diagonal_(1.0)  # Ensure diagonal is exactly 1\n",
    "    \n",
    "    # Cholesky decomposition (requires positive definite matrix)\n",
    "    try:\n",
    "        L = torch.linalg.cholesky(R_gaussian + epsilon*torch.eye(d))\n",
    "    except RuntimeError as e:\n",
    "        raise ValueError(\"Invalid covariance: Resulting Gaussian correlation is not positive definite.\") from e\n",
    "    \n",
    "    # Generate multivariate Gaussian sample\n",
    "    z = torch.randn(d)  # Standard normal sample\n",
    "    gaussian_sample = L @ z  # Shape: (d,)\n",
    "    \n",
    "    # Transform Gaussian to uniform using CDF\n",
    "    uniform_sample = 0.5 * (1 + torch.erf(gaussian_sample / math.sqrt(2)))  # Shape: (d,)\n",
    "    \n",
    "    # Scale to match desired covariance (adjust variances and covariances)\n",
    "    scale_factor = torch.sqrt(12 * torch.diag(covariance_matrix_uniform))\n",
    "    scaled_uniform = uniform_sample * scale_factor + 0.5 * (1 - scale_factor)\n",
    "    \n",
    "    return scaled_uniform.reshape(-1, 1)  # Return as column vector\n",
    "def compute_gaussian_covariance_from_uniform(covariance_matrix_unifrom, epsilon = 1e-4):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian covariance matrix from the uniform covariance matrix.\n",
    "    \n",
    "    Args:\n",
    "        covariance_matrix_unifrom (torch.Tensor): Covariance matrix of the uniforms, shape (d, d).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Gaussian covariance matrix, shape (d, d).\n",
    "    \"\"\"\n",
    "    # Compute Pearson correlation matrix of the uniforms\n",
    "    diag_var_u = torch.diag(covariance_matrix_unifrom)  # Variances of uniforms (should be ~1/12)\n",
    "    std_u = torch.sqrt(diag_var_u)    # Standard deviations of uniforms\n",
    "    outer_std = torch.outer(std_u, std_u)\n",
    "    R_u = covariance_matrix_unifrom / outer_std         # Pearson correlation matrix of uniforms\n",
    "\n",
    "    # Compute Gaussian correlation matrix\n",
    "    R_n = 2 * torch.sin((math.pi / 6) * R_u)\n",
    "\n",
    "    # Ensure diagonal is exactly 1 (due to numerical precision)\n",
    "    R_n.fill_diagonal_(1.0)\n",
    "    covariance_matrix_gaussian =  R_n + torch.eye(R_u.shape[0])\n",
    "    return covariance_matrix_gaussian\n",
    "\n",
    "def compute_pixel_cdf_map(images, max_pixel_value = 255):\n",
    "    \"\"\"\n",
    "    Compute the empirical CDF for each pixel position across all images.\n",
    "    \n",
    "    Args:\n",
    "        images (torch.Tensor): Tensor of shape (num_samples, pixel_size, pixel_size),\n",
    "                               containing grayscale images with integer pixel values (0-255).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (pixel_size**2, 256), where each row contains\n",
    "                      the empirical CDF for a given pixel position.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_samples, pixel_size, _ = images.shape\n",
    "    # max_pixel_value = images.max()\n",
    "    \n",
    "    # Flatten images along the pixel dimension\n",
    "    images_flat = images.view(num_samples, -1)  # Shape: (num_samples, pixel_size**2)\n",
    "    \n",
    "    # Initialize the CDF map\n",
    "    cdf_map = torch.zeros((images_flat.shape[1], max_pixel_value + 1), device=images.device)\n",
    "    \n",
    "    # Compute the empirical CDF for each pixel location\n",
    "    for i in range(images_flat.shape[1]):\n",
    "        pixel_values = images_flat[:, i]  # All values for a specific pixel position\n",
    "        hist = torch.bincount(pixel_values, minlength=max_pixel_value + 1).float()  # Count occurrences\n",
    "        cdf_map[i] = hist.cumsum(dim=0) / num_samples  # Normalize to get CDF\n",
    "    \n",
    "    return cdf_map\n",
    "import torch\n",
    "\n",
    "def transform_dataset_to_uniform_distribution(images, cdf_map, reverse=False):\n",
    "    \"\"\"\n",
    "    Transform the dataset into a uniform [0,1] dataset using pixel-wise empirical CDF.\n",
    "    If reverse=True, transform back to the original space.\n",
    "\n",
    "    Args:\n",
    "        images (torch.Tensor): Input dataset of shape (num_samples, pixel_size, pixel_size).\n",
    "                               If reverse=False, dtype=torch.uint8; if reverse=True, dtype=torch.float32.\n",
    "        cdf_map (torch.Tensor): Empirical CDF map of shape (pixel_size**2, 256).\n",
    "        reverse (bool): If True, transform back to the original pixel space.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Transformed dataset of the same shape as input.\n",
    "    \"\"\"\n",
    "    num_samples, pixel_size, _ = images.shape\n",
    "    images_flat = images.view(num_samples, -1)  # Flatten for batch processing\n",
    "    cdf_map = cdf_map.to(images.device)  # Ensure cdf_map is on the same device\n",
    "\n",
    "    if not reverse:\n",
    "        # Forward transformation: Map pixel values to uniform [0,1]\n",
    "        images_flat = images_flat.long()  # Convert to long for indexing\n",
    "        transformed_images_flat = cdf_map[torch.arange(images_flat.shape[1], device=images.device).unsqueeze(0), images_flat]\n",
    "        \n",
    "        # Reshape and ensure float output\n",
    "        transformed_images = transformed_images_flat.view(num_samples, pixel_size, pixel_size).to(torch.float32)\n",
    "        # gaussian_images = torch.erfinv(2 * transformed_images - 1) * torch.sqrt(torch.tensor(2.0))\n",
    "        return transformed_images#, gaussian_images\n",
    "\n",
    "    else:\n",
    "        # Reverse transformation: Map uniform values back to pixel space\n",
    "        pixel_positions = torch.arange(cdf_map.shape[0], device=images.device)\n",
    "\n",
    "        # Use `searchsorted` for **each** pixel position separately\n",
    "        transformed_images_flat = torch.stack([\n",
    "            torch.searchsorted(cdf_map[i], images_flat[:, i], right=True) for i in range(cdf_map.shape[0])\n",
    "        ], dim=1).clip(0, 255)  # Shape (num_samples, pixel_size**2)\n",
    "\n",
    "        # Reshape and ensure byte output\n",
    "        transformed_images = transformed_images_flat.view(num_samples, pixel_size, pixel_size).to(torch.uint8)\n",
    "        return transformed_images\n",
    "\n",
    "def plot_histogram_of_data(data: torch.Tensor, bins: int = 50, color: str = 'blue',\n",
    "                     title: str = 'Empirical PDF', xlabel: str = 'Value', \n",
    "                     ylabel: str = 'Density') -> tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    Plot the histogram of a flattened tensor (assumed to represent CDF values).\n",
    "\n",
    "    Args:\n",
    "        data: Input tensor of shape (N,) or (batch_size, ...). Will be flattened.\n",
    "        bins: Number of histogram bins.\n",
    "        color: Histogram color.\n",
    "        title: Plot title.\n",
    "        xlabel: x-axis label.\n",
    "        ylabel: y-axis label.\n",
    "\n",
    "    Returns:\n",
    "        Matplotlib figure and axes.\n",
    "    \"\"\"\n",
    "    # Convert to numpy and flatten\n",
    "    data_np = data.flatten().cpu().numpy()  # Handles GPU tensors\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 5), tight_layout=True)\n",
    "    ax.hist(data_np, bins=bins, density=True, alpha=0.6, color=color, label='Empirical PDF')\n",
    "    ax.axhline(1.0, color='black', linestyle='--', label='Uniform PDF')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def min_max_normalization(data):\n",
    "    return (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "def transform_data_to_have_the_true_covariance(input_images, covariance_matrix_true):\n",
    "    \"\"\"\n",
    "    Transforms a batch of input images such that their covariance matches the given real data covariance.\n",
    "\n",
    "    Args:\n",
    "        input_images (torch.Tensor): Fake images sampled from a input distribution (num_samples, p, p).\n",
    "        covariance_matrix_true (torch.Tensor): Target covariance matrix of real data (p*p, p*p).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Transformed input images with the same covariance as real data (num_samples, p, p).\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples, pixel_size, _ = input_images.shape\n",
    "    p2 = pixel_size ** 2  # Flattened pixel count\n",
    "\n",
    "    # Flatten images\n",
    "    input_images_flat = input_images.view(num_samples, p2)\n",
    "\n",
    "\n",
    "    # Compute empirical covariance of input samples\n",
    "    mean_input = input_images_flat.mean(dim=0, keepdim=True)\n",
    "    centered_input = input_images_flat - mean_input\n",
    "    covariance_input = (centered_input.T @ centered_input) / (num_samples - 1)\n",
    "\n",
    "    # Compute Cholesky decomposition (or Eigen decomposition)\n",
    "    L_real = torch.linalg.cholesky(covariance_matrix_true)  # L_real @ L_real.T = covariance_matrix_true\n",
    "    L_input = torch.linalg.cholesky(covariance_input)  # L_input @ L_input.T = covariance_input\n",
    "\n",
    "    # Compute correction matrix A\n",
    "    correction_matrix = L_real @ torch.linalg.inv(L_input)\n",
    "\n",
    "    # Apply correction matrix\n",
    "    transformed_images_flat = (input_images_flat - mean_input) @ correction_matrix.T\n",
    "    transformed_images = transformed_images_flat.view(num_samples, pixel_size, pixel_size)\n",
    "\n",
    "    return transformed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 9\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def generate_uniform_random_cifar_rgb_per_class(label_idx, num_samples = 5000, set_name = 'cifar-rgb', epsilon = 0):\n",
    "    num_bands = 1\n",
    "    if set_name == 'mnist':\n",
    "        images = mnist_images\n",
    "        train_images = mnist_images[mnist_labels == label_idx]\n",
    "        pixel_size = images.shape[2]\n",
    "        train_images = train_images.unsqueeze(1)\n",
    "    elif set_name == 'fashion-mnist':\n",
    "        images = fashion_mnist_images[fashion_mnist_labels == label_idx]\n",
    "        pixel_size = images.shape[2]\n",
    "        train_images = fashion_mnist_images[fashion_mnist_labels == label_idx]\n",
    "        train_images = train_images.unsqueeze(1)\n",
    "    elif set_name == 'cifar-gray':\n",
    "        pixel_size = cifar_images.shape[2]\n",
    "        train_images = cifar_gray[torch.tensor(cifar_labels) == label_idx,:,:].squeeze(dim=1)\n",
    "        train_images = train_images.unsqueeze(1)\n",
    "    elif set_name == 'cifar-rgb':\n",
    "        num_bands = 3\n",
    "        pixel_size = cifar_images.shape[2]\n",
    "        train_images = cifar_images[torch.tensor(cifar_labels) == label_idx,:,:,:]\n",
    "        # train_images = torch.zeros(5000, 3, pixel_size, pixel_size)\n",
    "        # for band_idx in range(num_bands): \n",
    "            # train_images = cifar_images[torch.tensor(cifar_labels) == label_idx,band_idx,:,:].squeeze(dim=1)\n",
    "    else: return [], []\n",
    "    # print(train_images.shape)\n",
    "    num_samples = min(num_samples, train_images.shape[0])\n",
    "    uniform_real_images = torch.zeros(num_samples, num_bands, pixel_size, pixel_size)\n",
    "    uniform_fake_images = torch.zeros(num_samples, num_bands, pixel_size, pixel_size)\n",
    "    covariance_matrix_real_uniform = torch.zeros(num_bands, pixel_size**2, pixel_size**2)\n",
    "    covariance_matrix_real_gaussian = torch.zeros(num_bands, pixel_size**2, pixel_size**2)\n",
    "    covariance_matrix_real_images = torch.zeros(num_bands, pixel_size**2, pixel_size**2)\n",
    "    correction_matrix = torch.zeros(num_bands, pixel_size**2, pixel_size**2)\n",
    "    eigen_vector_data = torch.zeros(num_bands, pixel_size**2, pixel_size**2)\n",
    "    mean_vecotr_data = torch.zeros(num_bands, pixel_size**2)\n",
    "    covariance_matrix_uniform = torch.zeros(num_bands, pixel_size**2, pixel_size**2)\n",
    "    synthetic_image = torch.zeros(num_bands, pixel_size, pixel_size)\n",
    "    # pixel_cdf_map = [0]*num_bands\n",
    "    pixel_cdf_map = torch.zeros(num_bands, pixel_size**2, 256)\n",
    "    synthetic_images = torch.zeros(num_bands, num_samples, pixel_size, pixel_size)\n",
    "    for band_idx in range(num_bands):\n",
    "\n",
    "        uniform_real_images[:, band_idx, :, :]= transform_dataset_to_uniform_distribution(train_images[:, band_idx, :, :], pixel_cdf_map[band_idx, :], reverse=False)\n",
    "        # uniform_real_images, _ = transform_dataset_to_uniform_and_gaussian_vectorized(train_images)\n",
    "        covariance_matrix_real_images[band_idx, :, :], _ = compute_mean_and_covariance_in_data_space(train_images[:, band_idx, :, :], epsilon=epsilon)\n",
    "        covariance_matrix_real_uniform[band_idx, :, :], _ = compute_mean_and_covariance_in_data_space(uniform_real_images[:, band_idx,: ,:], epsilon=epsilon)\n",
    "        covariance_matrix_real_gaussian[band_idx, :, :] = compute_gaussian_covariance_from_uniform(covariance_matrix_real_uniform[band_idx, :, :], epsilon = epsilon)\n",
    "\n",
    "\n",
    "        # uniform_fake_images = generate_random_uniform_images(covariance_matrix_real_uniform, num_samples = num_samples)\n",
    "        uniform_fake_images[:, band_idx, :, :] = generate_random_uniform_images(covariance_matrix_real_uniform[band_idx, :, :], num_samples = num_samples)\n",
    "        uniform_fake_images[:, band_idx, :, :] = generate_random_uniform_images_from_gaussian(covariance_matrix_real_gaussian[band_idx, :, :], num_samples= num_samples)\n",
    "        # uniform_fake_images[:, band_idx, :, :] = transform_data_to_have_the_true_covariance(uniform_fake_images[:, band_idx, :, :], covariance_matrix_real_uniform[band_idx, :, :])\n",
    "        # uniform_fake_images = (uniform_fake_images - uniform_fake_images.min()) / (uniform_fake_images.max() - uniform_fake_images.min())\n",
    "\n",
    "        # uniform_fake_images = generate_random_uniform_images(torch.eye(pixel_size**2), num_samples = num_samples)\n",
    "        # uniform_fake_images = torch.rand(num_samples,pixel_size,pixel_size)\n",
    "\n",
    "        synthetic_images[band_idx, :, :, :] = transform_dataset_to_uniform_distribution(uniform_fake_images[:, band_idx, :, :], pixel_cdf_map[band_idx, :], reverse=True)\n",
    "        # print(synthetic_images)\n",
    "        # synthetic_images[band_idx, :, :, :] = transform_data_to_have_the_true_covariance(synthetic_images[band_idx, :, :].float(), covariance_matrix_real_images[band_idx, :, :].float())\n",
    "\n",
    "    return uniform_real_images, uniform_fake_images, synthetic_images\n",
    " \n",
    "def plot_synthetic_images(synthetic_images, label_idx, set_name = 'cifar-rgb', number_of_rows_and_columns = 10):\n",
    "    num_bands = synthetic_images.shape[0]\n",
    "    pixel_size = synthetic_images.shape[-1]\n",
    "    fig, axs = plt.subplots(number_of_rows_and_columns, number_of_rows_and_columns, figsize=(number_of_rows_and_columns, number_of_rows_and_columns))\n",
    "    idx = -1\n",
    "    synthetic_image = synthetic_images[:,0,:,:]\n",
    "    correction_matrix = torch.zeros(num_bands, pixel_size**2, pixel_size**2)\n",
    "    mean_vecotr_data = torch.zeros(num_bands, pixel_size**2)\n",
    "    for idx_i in range(number_of_rows_and_columns):\n",
    "        for idx_j in range (0, number_of_rows_and_columns):\n",
    "            idx += 1\n",
    "            for band_idx in range(num_bands):\n",
    "                tmp = synthetic_images[band_idx, idx, :, :]\n",
    "                synthetic_image[band_idx, :, :] = tmp\n",
    "\n",
    "            if num_bands == 3:\n",
    "                axs[idx_i,idx_j].imshow(synthetic_image.permute(1,2,0))\n",
    "            else:\n",
    "                axs[idx_i,idx_j].imshow(synthetic_image.permute(1,2,0), cmap='gray')\n",
    "\n",
    "            axs[idx_i,idx_j].set_xticks([])  # Hide x-ticks\n",
    "            axs[idx_i,idx_j].set_yticks([])  # Hide x-ticks\n",
    "    fig.suptitle('Fake ' + set_name + ' images')\n",
    "    plt.savefig('fake-' + set_name + '-class-'+ str(label_idx) +'.pdf')\n",
    "    plt.close(fig)  # Close figure to free memory\n",
    "\n",
    "num_samples = 10000\n",
    "num_classes = 10\n",
    "uniform_cifar_real_images = torch.zeros(num_samples*num_classes, 3, 32, 32)\n",
    "uniform_cifar_fake_images = torch.zeros(num_samples*num_classes, 3, 32, 32)\n",
    "set_name = 'mnist'\n",
    "# set_name = 'fashion-mnist'\n",
    "# set_name = 'cifar-rgb'\n",
    "set_name = 'cifar-gray'\n",
    "ifPlot = True\n",
    "# if num_samples <= 100: ifPlot = True\n",
    "for label_idx in range(num_classes): \n",
    "    print(f\"Class: {label_idx}\" , end=\"\\r\")\n",
    "    # uniform_cifar_real_images[label_idx*5000:(1+label_idx)*5000,:,:,:], uniform_cifar_fake_images[label_idx*5000:(1+label_idx)*5000] = generate_uniform_random_cifar_rgb_per_class(label_idx, num_samples, ifPlot = False)\n",
    "    _, _,synthetic_images = generate_uniform_random_cifar_rgb_per_class(label_idx, num_samples = num_samples, set_name = set_name, epsilon = 1e-8)\n",
    "    plot_synthetic_images(synthetic_images, label_idx, set_name=set_name, number_of_rows_and_columns=5)\n",
    "os.system(f\"pdftk fake-{set_name}-class-*.pdf output fake-{set_name}.pdf\")\n",
    "os.system(f\"rm fake-{set_name}*class*\")\n",
    "    # print(tmp1.shape, tmp2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, dtype=torch.uint8) tensor(255, dtype=torch.uint8)\n",
      "tensor(8.3898)\n",
      "tensor(453503.8125)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_idx = 1\n",
    "epsilon = 1e-4\n",
    "set_name = 'cifar-gray'\n",
    "pixel_size = cifar_images.shape[-1]\n",
    "train_images = cifar_gray[torch.tensor(cifar_labels) == label_idx,:,:].squeeze(dim=1)\n",
    "\n",
    "# set_name == 'fashion-mnist'\n",
    "# images = fashion_mnist_images[fashion_mnist_labels == label_idx]\n",
    "# pixel_size = images.shape[2]\n",
    "# train_images = fashion_mnist_images[fashion_mnist_labels == label_idx]\n",
    "# num_samples = 5000\n",
    "# train_images = torch.randint(0,255,(num_samples, pixel_size, pixel_size), dtype=torch.uint8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_samples = train_images.shape[0]\n",
    "num_samples = min(num_samples, train_images.shape[0])\n",
    "pixel_cdf_map = compute_pixel_cdf_map(train_images.squeeze())\n",
    "# transformed_images = map_images_using_cdf(train_images.squeeze().reshape(-1,pixel_size,pixel_size), pixel_cdf_map)\n",
    "uniform_real_images = transform_dataset_to_uniform_distribution(train_images, pixel_cdf_map, reverse=False)\n",
    "# uniform_real_images, _ = transform_dataset_to_uniform_and_gaussian_vectorized(train_images)\n",
    "covariance_matrix_real_images, _ = compute_mean_and_covariance_in_data_space(train_images, epsilon=epsilon)\n",
    "covariance_matrix_real_uniform, _ = compute_mean_and_covariance_in_data_space(uniform_real_images, epsilon=epsilon)\n",
    "covariance_matrix_real_gaussian = compute_gaussian_covariance_from_uniform(covariance_matrix_real_uniform, epsilon = epsilon)\n",
    "\n",
    "\n",
    "# uniform_fake_images = generate_random_uniform_images(covariance_matrix_real_uniform, num_samples = num_samples)\n",
    "uniform_fake_images = generate_random_uniform_images(covariance_matrix_real_uniform, num_samples = num_samples)\n",
    "uniform_fake_images = generate_random_uniform_images_from_gaussian(covariance_matrix_real_gaussian, num_samples= num_samples)\n",
    "# uniform_fake_images = uniform_real_images\n",
    "# uniform_fake_images = transform_data_to_have_the_true_covariance(uniform_fake_images, covariance_matrix_real_uniform)\n",
    "# uniform_fake_images = (uniform_fake_images - uniform_fake_images.min()) / (uniform_fake_images.max() - uniform_fake_images.min())\n",
    "\n",
    "# uniform_fake_images = generate_random_uniform_images(torch.eye(pixel_size**2), num_samples = num_samples)\n",
    "# uniform_fake_images = torch.rand(num_samples,pixel_size,pixel_size)\n",
    "\n",
    "covariance_matrix_fake_uniform, _ = compute_mean_and_covariance_in_data_space(uniform_fake_images, epsilon=epsilon)\n",
    "# plot_histogram_of_data(uniform_fake_images)\n",
    "# print(uniform_fake_images.min(), uniform_fake_images.max())\n",
    "plot_histogram_of_data(uniform_fake_images)\n",
    "\n",
    "synthetic_images = transform_dataset_to_uniform_distribution(uniform_fake_images, pixel_cdf_map, reverse=True)\n",
    "# synthetic_images = transform_data_to_have_the_true_covariance(synthetic_images.float(), covariance_matrix_real_images.float())\n",
    "# synthetic_images = 255*min_max_normalization(synthetic_images)\n",
    "print(synthetic_images.min(), synthetic_images.max())\n",
    "plot_histogram_of_data(synthetic_images)\n",
    "\n",
    "plot_synthetic_images(synthetic_images.unsqueeze(0), label_idx, set_name=set_name, number_of_rows_and_columns=5)\n",
    "\n",
    "\n",
    "covariance_matrix_fake_uniform, _ = compute_mean_and_covariance_in_data_space(uniform_fake_images, epsilon=epsilon)\n",
    "print(torch.norm(covariance_matrix_real_uniform-covariance_matrix_fake_uniform))\n",
    "# print(torch.diag(covariance_matrix_fake_uniform)*12)\n",
    "\n",
    "covariance_matrix_fake_images, _ = compute_mean_and_covariance_in_data_space(synthetic_images, epsilon=epsilon)\n",
    "print(torch.norm(covariance_matrix_real_images-covariance_matrix_fake_images))\n",
    "\n",
    "      \n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_uniform_images(covariance_matrix_real_uniform, num_samples):\n",
    "    \"\"\"\n",
    "    Generates samples from a uniform distribution with the given covariance matrix.\n",
    "\n",
    "    Args:\n",
    "        covariance_matrix_real_uniform (torch.Tensor): Desired covariance matrix of shape (p*p, p*p).\n",
    "        num_samples (int): Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Uniformly distributed images with the specified covariance matrix,\n",
    "                      shape (num_samples, p, p).\n",
    "    \"\"\"\n",
    "    p2 = covariance_matrix_real_uniform.shape[0]  # Number of pixels squared (p*p)\n",
    "    pixel_size = int(p2 ** 0.5)  # Extract pixel size (p)\n",
    "\n",
    "    # Compute Cholesky decomposition (or an alternative decomposition if needed)\n",
    "    L = torch.linalg.cholesky(covariance_matrix_real_uniform)\n",
    "\n",
    "    # Generate uniform random samples in [0,1]\n",
    "    uniform_samples = torch.rand(num_samples, p2)\n",
    "\n",
    "    # Apply transformation to induce the desired covariance\n",
    "    transformed_uniform_samples = torch.matmul(uniform_samples, L.T)\n",
    "\n",
    "    # Rescale back to [0,1] (Min-Max normalization)\n",
    "    min_val, max_val = transformed_uniform_samples.min(), transformed_uniform_samples.max()\n",
    "    transformed_uniform_samples = (transformed_uniform_samples - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Reshape into images\n",
    "    transformed_uniform_images = transformed_uniform_samples.view(num_samples, pixel_size, pixel_size)\n",
    "\n",
    "    return transformed_uniform_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lp_tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
