{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 20000\n",
    "set_size = 60000\n",
    "batch_size = 2\n",
    "sample_dir = 'samples'\n",
    "save_dir = 'save'\n",
    "\n",
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Image processing\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "# Data loader\n",
    "# data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "#                                          batch_size=batch_size, \n",
    "#                                          shuffle=True)\n",
    "# Download MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transform, \n",
    "                               download=True)\n",
    "# Split original training set into 70% train and 30% validation\n",
    "train_size = int(0.7 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Select a random image from the new training set\n",
    "random_index = np.random.randint(len(train_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "mnist_images = train_dataloader.dataset.dataset.data\n",
    "mnist_labels = train_dataset.dataset.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 20000\n",
    "set_size = 60000\n",
    "batch_size = 2\n",
    "sample_dir = 'samples'\n",
    "save_dir = 'save'\n",
    "\n",
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Image processing\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels\n",
    "                                     std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "# Data loader\n",
    "# data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "#                                          batch_size=batch_size, \n",
    "#                                          shuffle=True)\n",
    "# Download MNIST dataset\n",
    "mnist_dataset = datasets.FashionMNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transform, \n",
    "                               download=True)\n",
    "# Split original training set into 70% train and 30% validation\n",
    "train_size = int(0.7 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Select a random image from the new training set\n",
    "random_index = np.random.randint(len(train_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "fashion_mnist_images = train_dataloader.dataset.dataset.data\n",
    "fashion_mnist_labels = train_dataset.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                # Convert to tensor\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert RGB to Grayscale\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                               train=True, \n",
    "                               transform=transform, \n",
    "                               download=True)\n",
    "\n",
    "# # Split original training set into 70% train and 30% validation\n",
    "train_size = int(0.7 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# # Select a random image from the new training set\n",
    "# random_index = np.random.randint(len(train_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "cifar_images = torch.tensor(train_dataloader.dataset.dataset.data).permute(0, 3, 1, 2) / 255.0  # Convert to Tensor & Normalize\n",
    "\n",
    "# cifar_images = train_dataloader.dataset.dataset.data\n",
    "cifar_labels = train_dataset.dataset.targets\n",
    "\n",
    "to_grayscale = transforms.Grayscale(num_output_channels=1)\n",
    "cifar_gray = torch.stack([to_grayscale(img) for img in cifar_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0\r"
     ]
    }
   ],
   "source": [
    "def compute_cdf_mapping(images):\n",
    "    \"\"\"\n",
    "    Compute the CDF mapping for each pixel across all images.\n",
    "    \"\"\"\n",
    "    \n",
    "    vector_size = images.shape[1]\n",
    "    images = images.view(-1, vector_size)  # Flatten images\n",
    "    sorted_pixels, _ = torch.sort(images, dim=0)\n",
    "    cdf = torch.linspace(0, 1, images.shape[0])\n",
    "    pixel_cdf_map = {}\n",
    "    \n",
    "    for i in range(vector_size):\n",
    "        pixel_cdf_map[i] = (sorted_pixels[:, i], cdf)\n",
    "    \n",
    "    return pixel_cdf_map\n",
    "\n",
    "def transform_original_to_uniform(image, pixel_cdf_map, reverse=False):\n",
    "    \"\"\"\n",
    "    Transform an image using the pixel CDF mapping.\n",
    "    \"\"\"\n",
    "    image = image.view(-1)  # Flatten image\n",
    "    transformed_image = torch.zeros_like(image, dtype=torch.float32)\n",
    "    \n",
    "    pixel_size = int(image.shape[0] ** 0.5)\n",
    "    for i in range(pixel_size * pixel_size):\n",
    "        pixel_values, cdf_values = pixel_cdf_map[i]\n",
    "        \n",
    "        if not reverse:\n",
    "            # Forward transformation (find CDF value for each pixel)\n",
    "            indices = torch.searchsorted(pixel_values, image[i])\n",
    "            transformed_image[i] = cdf_values[min(indices, len(cdf_values) - 1)]\n",
    "        else:\n",
    "            # Reverse transformation (find original pixel from CDF value)\n",
    "            indices = torch.searchsorted(cdf_values, image[i])\n",
    "            transformed_image[i] = pixel_values[min(indices, len(pixel_values) - 1)]\n",
    "    \n",
    "    transformed_image = transformed_image.view(pixel_size, pixel_size)  # Reshape back to image size\n",
    "    return transformed_image\n",
    "\n",
    "def compute_mean_and_covariance(images, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Compute the covariance matrix of the dataset, ensuring it is positive definite.\n",
    "    \"\"\"\n",
    "    images = images.float()  # Ensure floating-point type\n",
    "    images = images.view(images.shape[0], -1)  # Flatten images\n",
    "    mean_vector = torch.mean(images, dim=0, keepdim=True)\n",
    "    centered_images = images - mean_vector\n",
    "    covariance_matrix = torch.matmul(centered_images.T, centered_images) / (images.shape[0] - 1)\n",
    "    \n",
    "    # Regularization: Add a small identity matrix to ensure positive definiteness\n",
    "    covariance_matrix += epsilon * torch.eye(covariance_matrix.shape[0])\n",
    "    \n",
    "    return covariance_matrix, mean_vector\n",
    "\n",
    "def generate_random_uniform_images_from_gaussian(covariance_matrix_gaussian, num_samples = 1):\n",
    "    vector_size = covariance_matrix_gaussian.size(0)\n",
    "    num_pixels = int(vector_size**0.5)\n",
    "    mean = torch.zeros(vector_size, device=covariance_matrix_gaussian.device)\n",
    "    \n",
    "    # Create the multivariate normal distribution\n",
    "    mvn = torch.distributions.MultivariateNormal(\n",
    "        loc=mean,\n",
    "        covariance_matrix=covariance_matrix_gaussian\n",
    "    )\n",
    "    \n",
    "    # Generate samples\n",
    "    gaussian_samples = mvn.rsample((num_samples,))\n",
    "    uniform_samples = 0.5 * (1 + torch.erf(gaussian_samples / np.sqrt(2)))  # Convert Gaussian to Uniform [0,1]\n",
    "    return uniform_samples.reshape(num_samples, num_pixels, num_pixels)\n",
    "\n",
    "\n",
    "def generate_random_uniform_images(covariance_matrix, num_samples = 1):\n",
    "    \"\"\"\n",
    "    Generate samples from a multivariate normal distribution with mean 0 and given covariance matrix.\n",
    "    \n",
    "    Args:\n",
    "        num_samples (int): Number of samples to generate.\n",
    "        covariance_matrix (torch.Tensor): Covariance matrix of shape (num_pixels, num_pixels).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (num_samples, num_pixels) containing the samples.\n",
    "    \"\"\"\n",
    "    vector_size = covariance_matrix.size(0)\n",
    "    num_pixels = int(vector_size**0.5)\n",
    "    mean = torch.zeros(vector_size, device=covariance_matrix.device)\n",
    "    \n",
    "    # Create the multivariate normal distribution\n",
    "    mvn = torch.distributions.MultivariateNormal(\n",
    "        loc=mean,\n",
    "        covariance_matrix=covariance_matrix\n",
    "    )\n",
    "    \n",
    "    # Generate samples\n",
    "    gaussian_samples = mvn.rsample((num_samples,))\n",
    "    uniform_samples = 0.5 * (1 + torch.erf(gaussian_samples / np.sqrt(2)))  # Convert Gaussian to Uniform [0,1]\n",
    "    return uniform_samples.reshape(num_samples, num_pixels, num_pixels)\n",
    "\n",
    "\n",
    "# def transform_dataset_to_uniform_vectorized(dataset):\n",
    "#     \"\"\"\n",
    "#     Transform an entire dataset of images (tensor of shape (n, p, p)) so that for each pixel location,\n",
    "#     the empirical distribution of pixel values becomes uniform in [0,1].\n",
    "    \n",
    "#     The transformation is performed using a double argsort to obtain the rank of each pixel value.\n",
    "    \n",
    "#     Args:\n",
    "#         dataset (torch.Tensor): Tensor of shape (n, p, p)\n",
    "        \n",
    "#     Returns:\n",
    "#         torch.Tensor: Transformed dataset with shape (n, p, p) where each pixel's value is in [0,1].\n",
    "#     \"\"\"\n",
    "#     n, p, _ = dataset.shape\n",
    "#     # Flatten images: shape (n, p*p)\n",
    "#     flat_data = dataset.view(n, -1)\n",
    "    \n",
    "#     # Compute the rank for each pixel across the dataset.\n",
    "#     # First argsort sorts the values; the second argsort recovers the rank.\n",
    "#     ranks = flat_data.argsort(dim=0).argsort(dim=0).float()\n",
    "    \n",
    "#     # Normalize ranks to [0,1]\n",
    "#     uniform_flat = ranks / (n - 1)\n",
    "    \n",
    "#     # Reshape back to (n, p, p)\n",
    "#     return uniform_flat.view(n, p, p)\n",
    "# import torch\n",
    "\n",
    "def transform_dataset_to_uniform_and_gaussian_vectorized(dataset):\n",
    "    \"\"\"\n",
    "    Transform an entire dataset of images (tensor of shape (n, p, p)) so that for each pixel location,\n",
    "    the empirical distribution of pixel values becomes uniform in [0,1]. Then, using the relation\n",
    "    z = sqrt(2) * erfinv(2u - 1), convert the uniform dataset into a Gaussian distributed dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (torch.Tensor): Tensor of shape (n, p, p)\n",
    "        \n",
    "    Returns:\n",
    "        uniform_dataset (torch.Tensor): Transformed dataset with shape (n, p, p) with values in [0,1].\n",
    "        gaussian_dataset (torch.Tensor): Dataset transformed to be Gaussian distributed.\n",
    "    \"\"\"\n",
    "    n, p, _ = dataset.shape\n",
    "    # Flatten images: shape (n, p*p)\n",
    "    flat_data = dataset.view(n, -1)\n",
    "    \n",
    "    # Compute the rank for each pixel across the dataset via double argsort.\n",
    "    # First argsort sorts the values; the second argsort recovers the rank.\n",
    "    ranks = flat_data.argsort(dim=0).argsort(dim=0).float()\n",
    "    \n",
    "    # Normalize ranks to [0,1]\n",
    "    uniform_flat = ranks / (n - 1)\n",
    "    \n",
    "    # Reshape back to (n, p, p)\n",
    "    uniform_dataset = uniform_flat.view(n, p, p)\n",
    "    \n",
    "    # Convert the uniform dataset to Gaussian distributed data:\n",
    "    # For each pixel, apply: z = sqrt(2) * erfinv(2u - 1)\n",
    "    gaussian_dataset = torch.erfinv(2 * uniform_dataset - 1) * torch.sqrt(torch.tensor(2.0))\n",
    "    \n",
    "    return uniform_dataset, gaussian_dataset\n",
    "\n",
    "\n",
    "# def generate_uniform_iman_conover(covariance_matrix, n_samples=10000):\n",
    "#     \"\"\"\n",
    "#     Generate a sample from a d-dimensional distribution with Uniform(0,1) marginals \n",
    "#     whose covariance is approximately the given covariance_matrix.\n",
    "    \n",
    "#     The method uses the Iman–Conover procedure.\n",
    "    \n",
    "#     Args:\n",
    "#         covariance_matrix (np.ndarray): A d x d target covariance matrix.\n",
    "#             (For Uniform[0,1], the variance is 1/12, so the diagonal of covariance_matrix\n",
    "#             should be about 1/12.)\n",
    "#         n_samples (int): Number of samples to generate for the Iman–Conover adjustment.\n",
    "        \n",
    "#     Returns:\n",
    "#         sample (np.ndarray): A d x 1 column vector drawn from the adjusted Uniform(0,1) distribution.\n",
    "#     \"\"\"\n",
    "#     # Dimension (d) inferred from the covariance matrix\n",
    "#     d = covariance_matrix.shape[0]\n",
    "    \n",
    "#     # For Uniform[0,1], variance = 1/12. So the target correlation matrix is:\n",
    "#     R_target = covariance_matrix * 12.0\n",
    "    \n",
    "#     # Step 1: Generate independent Uniform(0,1) samples: shape (n_samples, d)\n",
    "#     U = np.random.uniform(0, 1, size=(n_samples, d))\n",
    "    \n",
    "#     # Step 2: Standardize each column of U\n",
    "#     U_std = (U - U.mean(axis=0)) / U.std(axis=0, ddof=1)\n",
    "    \n",
    "#     # Compute the empirical correlation matrix of U_std\n",
    "#     R_empirical = np.corrcoef(U_std, rowvar=False)\n",
    "    \n",
    "#     # Step 3: Compute Cholesky factors for the empirical and target correlation matrices\n",
    "#     L_empirical = np.linalg.cholesky(R_empirical)\n",
    "#     L_target = np.linalg.cholesky(R_target)\n",
    "    \n",
    "#     # Step 4: Compute the adjustment matrix\n",
    "#     A = L_target @ np.linalg.inv(L_empirical)\n",
    "    \n",
    "#     # Adjust the standardized samples\n",
    "#     Z = U_std @ A.T\n",
    "    \n",
    "#     # Step 5: For each variable (column), reassign values based on the ranks of Z,\n",
    "#     # so that the marginals remain Uniform(0,1) but the correlation structure is adjusted.\n",
    "#     U_adjusted = np.empty_like(U)\n",
    "#     for j in range(d):\n",
    "#         order = np.argsort(Z[:, j])\n",
    "#         sorted_vals = np.sort(U[:, j])\n",
    "#         U_adjusted[order, j] = sorted_vals\n",
    "    \n",
    "#     # Step 6: Choose one sample (e.g., the first row) and return it as a column vector.\n",
    "#     sample = U_adjusted[0, :].reshape(-1, 1)\n",
    "#     uniform_random_image = torch.tensor(sample).view(-1)\n",
    "#     return uniform_random_image\n",
    "\n",
    "# def generate_uniform_from_gaussian(covariance_matrix_uniform, epsilon=1e-4):\n",
    "#     \"\"\"\n",
    "#     Generates a column vector of uniform samples with the specified covariance matrix,\n",
    "#     using a multivariate Gaussian transformation. Handles PyTorch tensors and allows\n",
    "#     diagonal entries close to (but not exactly) 1/12.\n",
    "#     \"\"\"\n",
    "#     d = covariance_matrix_uniform.size(0)\n",
    "    \n",
    "#     # Compute standard deviations for each uniform variable\n",
    "#     sigma = torch.sqrt(torch.diag(covariance_matrix_uniform))  # Shape: (d,)\n",
    "    \n",
    "#     # Compute correlation matrix for the uniforms\n",
    "#     outer_sigma = torch.outer(sigma, sigma)\n",
    "#     R_uniform = covariance_matrix_uniform / outer_sigma  # Shape: (d, d)\n",
    "    \n",
    "#     # Compute Gaussian correlation matrix using adjusted formula\n",
    "#     R_gaussian = 2 * torch.sin((math.pi / 6) * R_uniform)\n",
    "#     R_gaussian.fill_diagonal_(1.0)  # Ensure diagonal is exactly 1\n",
    "    \n",
    "#     # Cholesky decomposition (requires positive definite matrix)\n",
    "#     try:\n",
    "#         L = torch.linalg.cholesky(R_gaussian + epsilon*torch.eye(d))\n",
    "#     except RuntimeError as e:\n",
    "#         raise ValueError(\"Invalid covariance: Resulting Gaussian correlation is not positive definite.\") from e\n",
    "    \n",
    "#     # Generate multivariate Gaussian sample\n",
    "#     z = torch.randn(d)  # Standard normal sample\n",
    "#     gaussian_sample = L @ z  # Shape: (d,)\n",
    "    \n",
    "#     # Transform Gaussian to uniform using CDF\n",
    "#     uniform_sample = 0.5 * (1 + torch.erf(gaussian_sample / math.sqrt(2)))  # Shape: (d,)\n",
    "    \n",
    "#     # Scale to match desired covariance (adjust variances and covariances)\n",
    "#     scale_factor = torch.sqrt(12 * torch.diag(covariance_matrix_uniform))\n",
    "#     scaled_uniform = uniform_sample * scale_factor + 0.5 * (1 - scale_factor)\n",
    "    \n",
    "#     return scaled_uniform.reshape(-1, 1)  # Return as column vector\n",
    "def uniform_to_gaussian_covariance(sigma_u, epsilon = 1e-4):\n",
    "    \"\"\"\n",
    "    Convert the covariance matrix of a multivariate uniform (0,1) distribution\n",
    "    to the covariance matrix of a multivariate normal distribution \n",
    "    after applying the inverse Gaussian CDF transformation.\n",
    "\n",
    "    Args:\n",
    "        sigma_u (torch.Tensor): Covariance matrix of the uniform distribution (d x d)\n",
    "\n",
    "    Returns:\n",
    "        sigma_n (torch.Tensor): Covariance matrix of the corresponding Gaussian distribution (d x d)\n",
    "    \"\"\"\n",
    "    # Compute the correlation matrix from Sigma_U\n",
    "    std_u = torch.sqrt(torch.diag(sigma_u))\n",
    "    corr_u = sigma_u / (std_u[:, None] * std_u[None, :])  # Normalize to get correlation\n",
    "\n",
    "    # Apply the Gaussian copula transformation\n",
    "    corr_n = 2 * torch.sin((torch.pi / 6) * corr_u) + torch.eye(sigma_u.shape[0])\n",
    "\n",
    "    # Convert back to covariance by assuming standard normal variance (1)\n",
    "    sigma_n = corr_n  # Since std_n = 1 for standard normal, no need to scale back\n",
    "\n",
    "    return sigma_n\n",
    "def compute_gaussian_covariance_from_uniform(Sigma_u, epsilon = 1e-4):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian covariance matrix from the uniform covariance matrix.\n",
    "    \n",
    "    Args:\n",
    "        Sigma_u (torch.Tensor): Covariance matrix of the uniforms, shape (d, d).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Gaussian covariance matrix, shape (d, d).\n",
    "    \"\"\"\n",
    "    # Compute Pearson correlation matrix of the uniforms\n",
    "    diag_var_u = torch.diag(Sigma_u)  # Variances of uniforms (should be ~1/12)\n",
    "    std_u = torch.sqrt(diag_var_u)    # Standard deviations of uniforms\n",
    "    outer_std = torch.outer(std_u, std_u)\n",
    "    R_u = Sigma_u / outer_std         # Pearson correlation matrix of uniforms\n",
    "\n",
    "    # Compute Gaussian correlation matrix\n",
    "    R_n = 2 * torch.sin((math.pi / 6) * R_u)\n",
    "\n",
    "    # Ensure diagonal is exactly 1 (due to numerical precision)\n",
    "    R_n.fill_diagonal_(1.0)\n",
    "    return R_n + torch.eye(R_u.shape[0])\n",
    "\n",
    "\n",
    "def generate_uniform_random_cifar_rgb_per_class(label_idx, num_samples = 5000, set_name = 'cifar-rgb', ifPlot = False):\n",
    "    num_bands = 1\n",
    "    if set_name == 'mnist':\n",
    "        images = mnist_images\n",
    "        train_images = mnist_images[mnist_labels == label_idx]\n",
    "        pixel_size = images.shape[2]\n",
    "    elif set_name == 'fashion-mnist':\n",
    "        images = fashion_mnist_images[fashion_mnist_labels == label_idx]\n",
    "        pixel_size = images.shape[2]\n",
    "        train_images = mnist_images[mnist_labels == label_idx]\n",
    "    elif set_name == 'cifar-gray':\n",
    "        pixel_size = cifar_images.shape[2]\n",
    "        # images = cifar_gray.squeeze(dim=1)\n",
    "        # images = cifar_gray[torch.tensor(cifar_labels) == label_idx].squeeze(dim=1)\n",
    "        # train_images = images.reshape(images.shape[0], images.shape[1]*images.shape[2])\n",
    "        train_images = cifar_gray[torch.tensour(cifar_labels) == label_idx,:,:].squeeze(dim=1)\n",
    "        print(train_images.shape)\n",
    "    elif set_name == 'cifar-rgb':\n",
    "        num_bands = 3\n",
    "        pixel_size = cifar_images.shape[2]\n",
    "        for band_idx in range(num_bands): train_images = cifar_images[torch.tensor(cifar_labels) == label_idx,band_idx,:,:].squeeze(dim=1)\n",
    "    else: return [], []\n",
    "    \n",
    "    uniform_real_images = torch.zeros(num_samples, num_bands, pixel_size, pixel_size)\n",
    "    uniform_fake_images = torch.zeros(num_samples, num_bands, pixel_size, pixel_size)\n",
    "    synthetic_image = torch.zeros(num_bands, pixel_size, pixel_size)\n",
    "    pixel_cdf_map = [0]*num_bands\n",
    "    for band_idx in range(num_bands):\n",
    "        pixel_cdf_map[band_idx] = compute_cdf_mapping(train_images.reshape(-1, pixel_size**2))\n",
    "        uniform_real_images[:, band_idx, :, :], _ = transform_dataset_to_uniform_and_gaussian_vectorized(train_images[0:num_samples])\n",
    "        # gaussian_images[gaussian_images > 1000] = 1000\n",
    "        covariance_matrix_uniform, mean_uniform = compute_mean_and_covariance(uniform_real_images[:, band_idx, :, :], epsilon = 0.000001)\n",
    "        covariance_matrix_gaussian = uniform_to_gaussian_covariance(covariance_matrix_uniform, epsilon = 1e-4) \n",
    "        covariance_matrix_gaussian = compute_gaussian_covariance_from_uniform(covariance_matrix_uniform, epsilon = 0) \n",
    "        # uniform_fake_images[:, band_idx,:,:] = generate_random_uniform_images(covariance_matrix_gaussian, num_samples = num_samples)\n",
    "        uniform_fake_images[:, band_idx,:,:] = generate_random_uniform_images_from_gaussian(covariance_matrix_uniform)\n",
    "\n",
    "\n",
    "    if ifPlot :\n",
    "        number_of_rows_or_columns = int(num_samples**0.5)\n",
    "        fig, axs = plt.subplots(number_of_rows_or_columns, number_of_rows_or_columns, figsize=(number_of_rows_or_columns, number_of_rows_or_columns))\n",
    "        idx = -1\n",
    "        for idx_i in range(number_of_rows_or_columns):\n",
    "            for idx_j in range (0, number_of_rows_or_columns):\n",
    "                idx += 1\n",
    "                print(f\"Class: {label_idx}, image {idx+1}/{number_of_rows_or_columns**2}\" , end=\"\\r\")\n",
    "                # uniform_random_image = generate_uniform_iman_conover(covariance_matrix_uniform, n_samples=2000)\n",
    "                # uniform_random_image =  generate_uniform_from_gaussian(covariance_matrix_uniform, epsilon=0.0001)\n",
    "                # uniform_random_image, gaussian_random_image = generate_random_image(covariance_matrix_uniform)\n",
    "                for band_idx in range(num_bands):\n",
    "                    uniform_fake_image = uniform_fake_images[idx, band_idx,:,:]\n",
    "                    synthetic_image[band_idx,:,:] = transform_original_to_uniform(uniform_fake_image.squeeze(), pixel_cdf_map[band_idx], reverse=True)\n",
    "\n",
    "                # _, gaussian_random_image = generate_random_image(covariance_matrix_gaussian, 0*mean_uniform)\n",
    "                # uniform_image_from_gaussian = transform_uniform_to_gaussian(gaussian_random_image, reverse=True)\n",
    "                # synthetic_image  = transform_original_to_uniform(uniform_image_from_gaussian, pixel_cdf_map, reverse=True)\n",
    "\n",
    "                if num_bands == 3:\n",
    "                    axs[idx_i,idx_j].imshow(synthetic_image.permute(1,2,0))\n",
    "                else:\n",
    "                    axs[idx_i,idx_j].imshow(synthetic_image.permute(1,2,0), cmap='gray')\n",
    "                # axs[idx_i,idx_j].imshow(cifar_images[np.random.randint(cifar_images.shape[0]),:,:].permute(1,2,0))\n",
    "\n",
    "                # axs[idx_i,idx_j].set_title('Uniform', fontsize=10)\n",
    "                axs[idx_i,idx_j].set_xticks([])  # Hide x-ticks\n",
    "                axs[idx_i,idx_j].set_yticks([])  # Hide x-ticks\n",
    "        fig.suptitle('Fake ' + set_name + ' images')\n",
    "        plt.savefig('fake-' + set_name + '-class-'+ str(label_idx) +'.pdf')\n",
    "        plt.close(fig)  # Close figure to free memory\n",
    "    return uniform_real_images, uniform_fake_images\n",
    "\n",
    "num_samples = 100\n",
    "num_classes = 2\n",
    "uniform_cifar_real_images = torch.zeros(num_samples*num_classes, 3, 32, 32)\n",
    "uniform_cifar_fake_images = torch.zeros(num_samples*num_classes, 3, 32, 32)\n",
    "set_name = 'mnist'\n",
    "set_name = 'cifar-rgb'\n",
    "ifPlot = False\n",
    "if num_samples <= 400: ifPlot = True\n",
    "for label_idx in range(num_classes): \n",
    "    print(f\"Class: {label_idx}\" , end=\"\\r\")\n",
    "    # uniform_cifar_real_images[label_idx*5000:(1+label_idx)*5000,:,:,:], uniform_cifar_fake_images[label_idx*5000:(1+label_idx)*5000] = generate_uniform_random_cifar_rgb_per_class(label_idx, num_samples, ifPlot = False)\n",
    "    tmp1, tmp2 = generate_uniform_random_cifar_rgb_per_class(label_idx, num_samples = num_samples, set_name = set_name, ifPlot = True)\n",
    "    # print(tmp1.shape, tmp2.shape)\n",
    "if ifPlot:\n",
    "    os.system(f\"pdftk fake-{set_name}-class-*.pdf output fake-{set_name}.pdf\")\n",
    "    os.system(f\"rm fake-{set_name}*class*\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lp_tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
